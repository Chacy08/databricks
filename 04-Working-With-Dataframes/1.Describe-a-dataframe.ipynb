{"cells":[{"cell_type":"markdown","source":["# Describe a DataFrame\n\nYour data processing in Azure Databricks is accomplished by defining Dataframes to read and process the Data.\n\nThis notebook will introduce how to read your data using Azure Databricks Dataframes."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bfaad047-e724-45f4-bb49-e68e9406a6c0","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#Introduction\n\n** Data Source **\n* One hour of Pagecounts from the English Wikimedia projects captured August 5, 2016, at 12:00 PM UTC.\n* Size on Disk: ~23 MB\n* Type: Compressed Parquet File\n* More Info: <a href=\"https://dumps.wikimedia.org/other/pagecounts-raw\" target=\"_blank\">Page view statistics for Wikimedia projects</a>\n\n**Technical Accomplishments:**\n* Develop familiarity with the `DataFrame` APIs\n* Introduce the classes...\n  * `SparkSession`\n  * `DataFrame` (aka `Dataset[Row]`)\n* Introduce the actions...\n  * `count()`"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"582e0d4d-c49c-463f-8546-5655b8fd0295","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"35dc62e4-3218-41ed-a85e-d961141d9a7f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bc69bf37-44dc-46d7-8437-c212afbbc2e1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) **The Data Source**\n\n* In this notebook, we will be using a compressed parquet \"file\" called **pagecounts** (~23 MB file from Wikipedia)\n* We will explore the data and develop an understanding of it as we progress.\n* You can read more about this dataset here: <a href=\"https://dumps.wikimedia.org/other/pagecounts-raw/\" target=\"_blank\">Page view statistics for Wikimedia projects</a>.\n\nWe can use **dbutils.fs.ls()** to view our data on the DBFS."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"539cbf7b-54dd-4dcb-b350-ab579ae73b81","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["(source, sasEntity, sasToken) = getAzureDataSource()\n\nspark.conf.set(sasEntity, sasToken)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a3e01f35-91cc-4644-9882-66290ed83a02","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["path = source + \"/wikipedia/pagecounts/staging_parquet_en_only_clean/\"\nfiles = dbutils.fs.ls(path)\ndisplay(files)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f62f58d-07e2-4eb1-8ed3-f78a561d2f52","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["As we can see from the files listed above, this data is stored in <a href=\"https://parquet.apache.org\" target=\"_blank\">Parquet</a> files which can be read in a single command, the result of which will be a `DataFrame`."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1ddb6668-33ce-42f7-9902-d8a42e62fe20","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Create a DataFrame\n* We can read the Parquet files into a `DataFrame`.\n* We'll start with the object **spark**, an instance of `SparkSession` and the entry point to Spark 2.0 applications.\n* From there we can access the `read` object which gives us an instance of `DataFrameReader`."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"74020868-ad23-4ad5-bf43-3036c4ac8ab2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["parquetDir = source + \"/wikipedia/pagecounts/staging_parquet_en_only_clean/\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6d7134f2-c0c5-43b0-a9f8-c29000e133d1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["pagecountsEnAllDF = (spark  # Our SparkSession & Entry Point\n  .read                     # Our DataFrameReader\n  .parquet(parquetDir)      # Returns an instance of DataFrame\n)\nprint(pagecountsEnAllDF)    # Python hack to see the data type"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8cb4fe6a-d412-4400-9603-f69a7ad089b6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) count()\n\nIf you look at the API docs, `count()` is described like this:\n> Returns the number of rows in the Dataset.\n\n`count()` will trigger a job to process the request and return a value.\n\nWe can now count all records in our `DataFrame` like this:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e827cad6-b644-488e-8846-4ba2fa0eb2bc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["total = pagecountsEnAllDF.count()\n\nprint(\"Record Count: {0:,}\".format( total ))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"de0a0822-ce3d-4bfe-a01d-2f8646a5a5a8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["That tells us that there are around 2 million rows in the `DataFrame`."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3f53f346-b98b-4c1a-a949-1da639770d9a","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Next steps\n\nStart the next lesson, [Use common DataFrame methods]($./2.Use-common-dataframe-methods)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ee65f6e2-fe81-4d03-8301-70accbd82a4d","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1.Describe-a-dataframe","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4208893588635871}},"nbformat":4,"nbformat_minor":0}
