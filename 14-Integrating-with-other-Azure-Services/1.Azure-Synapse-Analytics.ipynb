{"cells":[{"cell_type":"markdown","source":["# Reading and Writing to Azure Synapse Analytics\n**Technical Accomplishments:**\n- Access an Azure Synapse Analytics warehouse using the SQL Data Warehouse connector\n\n**Requirements:**\n- Databricks Runtime 4.0 or above\n- A database master key for Azure Synapse Analytics"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c8ccfdab-7af1-4952-854d-eb611bf52cb1","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7180d42a-b59d-439b-8ff8-7cba96129659","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3418935f-6759-4cf6-aa6d-a946047e84ed","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Azure Synapse Analytics\nAzure Synapse Analytics leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data.\n\nImport big data into Azure Synapse Analytics with simple PolyBase T-SQL queries, and then use MPP to run high-performance analytics.\n\nAs you integrate and analyze, the data warehouse will become the single version of truth your business can count on for insights."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6e1ca306-9962-440d-87d9-b4bd846ce05f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) SQL Data Warehouse Connector\n\n- Use Azure Blob Storage as an intermediary between Azure Databricks and Azure Synapse Analytics\n- In Azure Databricks: triggers Spark jobs to read and write data to Blob Storage\n- In Azure Synapse Analytics: triggers data loading and unloading operations, performed by **PolyBase**\n\n**Note:** The SQL DW connector is more suited to ETL than to interactive queries.  \nFor interactive and ad-hoc queries, data should be extracted into a Databricks Delta table."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6bd4a267-820f-4f47-81d1-096b4173491a","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["![Azure Databricks and Synapse Analytics](https://databricksdemostore.blob.core.windows.net/images/14-de-learning-path/databricks-synapse.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fb37c5f8-1b2c-4780-a84c-9c55c4cf62e4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Types of Connections in Azure Synapse Analytics\n\n### **Spark Driver to Azure Synapse Analytics**\nSpark driver connects to Azure Synapse Analytics via JDBC using a username and password.\n\n### **Spark Driver and Executors to Azure Blob Storage**\nSpark uses the **Azure Blob Storage connector** bundled in Databricks Runtime to connect to the Blob Storage container.\n  - Requires **`wasbs`** URI scheme to specify connection\n  - Requires **storage account access key** to set up connection\n    - Set in a notebook's session configuration, which doesn't affect other notebooks attached to the same cluster\n    - **`spark`** is the SparkSession object provided in the notebook\n\n### **Azure Synapse Analytics to Azure Blob Storage**\nSQL DW connector forwards the access key from notebook session configuration to an Azure Synapse Analytics instance over JDBC.\n  - Requires **`forwardSparkAzureStorageCredentials`** set to **`true`**\n  - Represents access key with a temporary <a href=\"https://docs.microsoft.com/en-us/sql/t-sql/statements/create-database-scoped-credential-transact-sql?view=sql-server-2017\" target=\"_blank\">database scoped credential</a> in the Azure Synapse Analytics instance\n  - Creates a database scoped credential before asking Azure Synapse Analytics to load or unload data, and deletes after loading/unloading is finished"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4457449-99d7-4cb3-9e7c-e500654e60ad","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Enabling access for a notebook session"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a9e5e3d1-fe5a-49e0-af0d-2fb369225f87","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["You can enable access for the lifetime of your notebook session to SQL Data Warehouse by executing the cell below. Be sure to replace the **\"name-of-your-storage-account\"** and **\"your-storage-key\"** values with your own before executing."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e8cbfaeb-25b6-447f-bb8b-8108ffeadf11","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["storage_account_name = \"name-of-your-storage-account\"\nstorage_account_key = \"your-storage-key\"\nstorage_container_name = \"data\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b2573ea9-8acb-430b-a3b2-07f4ee7e2e59","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["You will need the JDBC connection string for your Azure Synapse Analytics service. You should copy this value exactly as it appears in the Azure Portal.\n\n**Paste your JDBC connection string** into the empty quotation marks below. Please make sure you have replaced `{your_password_here}` with your SQL Server password."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7e22e146-f175-48e9-a88e-ccc9619b7d78","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["jdbcURI = \"\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e6e752db-0f68-4a86-809e-2bb84149c05b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Read from the Customer Table\n\nUse the SQL DW Connector to read data from the Customer Table.\n\nUse the read to define a tempory table that can be queried.\n\nNote the following options in the DataFrameReader in the cell below:\n* **`url`** specifies the JDBC connection to Azure Synapse Analytics\n* **`tempDir`** specifies the **`wasbs`** URI of the caching directory on the Azure Blob Storage container\n* **`forwardSparkAzureStorageCredentials`** is set to **`true`** to ensure that the Azure storage account access keys are forwarded from the notebook's session configuration to the Azure Synapse Analytics"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ec37be63-855d-47ee-809e-3b7cc50d0c7a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["cacheDir = \"wasbs://{}@{}.blob.core.windows.net/cacheDir\".format(storage_container_name, storage_account_name)\n\nspark_config_key = \"fs.azure.account.key.{}.blob.core.windows.net\".format(storage_account_name)\nspark_config_value = storage_account_key\n\nspark.conf.set(spark_config_key, spark_config_value)\n\ntableName = \"dbo.DimCustomer\"\n\ncustomerDF = (spark.read\n  .format(\"com.databricks.spark.sqldw\")\n  .option(\"url\", jdbcURI)\n  .option(\"tempDir\", cacheDir)\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n  .option(\"dbTable\", tableName)\n  .load())\n\ncustomerDF.createOrReplaceTempView(\"customer_data\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e9f5d628-553f-40e6-a0c9-112a8b57c459","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Use SQL queries to count the number of rows in the Customer table and to display table metadata."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"774bb364-57a1-40c8-8f92-c5ee4261ec28","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nselect count(*) from customer_data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ececcfcd-e609-4cab-9670-2bfaa1f9cd66","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\ndescribe customer_data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a10540e5-b3dc-4436-982c-69c6c5aac43c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Note that **`CustomerKey`** and **`CustomerAlternateKey`** use a very similar naming convention."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"77b86f05-e0ae-49cf-8540-4d3c91116ad0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nselect CustomerKey, CustomerAlternateKey from customer_data limit 10;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"617bc3ed-24c8-4ae4-a7fe-e3f3e9e5b33e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["When merging many new customers into this table, we may have issues with uniqueness in the **`CustomerKey`**. \n\nLet's redefine **`CustomerAlternateKey`** for stronger uniqueness using a <a href=\"https://en.wikipedia.org/wiki/Universally_unique_identifier\" target=\"_blank\">UUID</a>. To do this, we will define a UDF and use it to transform the **`CustomerAlternateKey`** column."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"541ef1ad-9fa3-4b27-a2d7-f93883b611af","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\nimport uuid\n\nuuidUdf = udf(lambda : str(uuid.uuid4()), StringType())\ncustomerUpdatedDF = customerDF.withColumn(\"CustomerAlternateKey\", uuidUdf())\ndisplay(customerUpdatedDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5536dc27-654c-4266-947a-70860cedda46","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Use the Polybase Connector to Write to the Staging Table\n\nUse the SQL DW Connector to write the updated customer table to a staging table.\n\nIt is best practice to update Azure Synapse Analytics via a staging table.\n\nNote the following options in the DataFrameWriter in the cell below:\n* **`url`** specifies the JDBC connection to Azure Synapse Analytics\n* **`tempDir`** specifies the **`wasbs`** URI of the caching directory on the Azure Blob Storage container\n* **`forwardSparkAzureStorageCredentials`** is set to **`true`** to ensure that the Azure storage account access keys are forwarded from the notebook's session configuration to Azure Synapse Analytics\n\nThese options are the same as those in the DataFrameReader above."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b05fb822-2687-47ba-9134-32f9ccc5033b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["(customerUpdatedDF.write\n  .format(\"com.databricks.spark.sqldw\")\n  .mode(\"overwrite\")\n  .option(\"url\", jdbcURI)\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n  .option(\"dbtable\", tableName + \"Staging\")\n  .option(\"tempdir\", cacheDir)\n  .save())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6834082e-9835-410b-8abf-fc18c826cf72","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Read From the New Staging Table\nUse the SQL DW Connector to read the new table we just wrote.\n\nUse the read to define a tempory table that can be queried."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c5622bd6-07bb-49db-9b1e-28b4fc945fb4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["customerTempDF = (spark.read\n  .format(\"com.databricks.spark.sqldw\")\n  .option(\"url\", jdbcURI)\n  .option(\"tempDir\", cacheDir)\n  .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n  .option(\"dbTable\", tableName + \"Staging\")\n  .load())\n\ncustomerTempDF.createOrReplaceTempView(\"customer_temp_data\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"85e25534-5658-455a-a84c-d137ffae3c0b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nselect CustomerKey, CustomerAlternateKey from customer_temp_data limit 10;"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e287f512-5781-40ef-ba0f-ccf63e59e357","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1.Azure-Synapse-Analytics","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4208893588637429}},"nbformat":4,"nbformat_minor":0}
